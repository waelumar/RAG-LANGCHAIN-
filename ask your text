
This script lets users input a custom text passage, 
and then ask questions about it. It uses Retrieval-Augmented Generation 
(RAG) powered by LangChain, Groq's LLaMA3, and HuggingFace embeddings.




from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from groq import Groq
from typing import List, Optional, Any, Mapping
from pydantic import Field


class GroqLLM(LLM):
    client: Any = Field(default_factory=lambda: Groq(api_key="groq api key"))
    model_name: str = "llama3-8b-8192" 
    system_prompt: str = "You are a helpful assistant."

    @property
    def _llm_type(self) -> str:
        return "groq"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt},
            ],
            max_tokens=512,
            temperature=0.7,
            stop=stop,
        )
        return response.choices[0].message.content

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model_name": self.model_name}



document_text=input(str("enter thr text"))

#split
text_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=10)
chunks=text_splitter.split_text(document_text)

#embedding
embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstores=FAISS.from_texts(chunks,embedding=embeddings)

#retriever
retriever=vectorstores.as_retriever()

#llm
llm=GroqLLM()

#qna setup
qna_chain=RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever
    )

question=str(input("ask your question "))
answer = qna_chain.run(question)

print("question",question)
print("answer",answer)

