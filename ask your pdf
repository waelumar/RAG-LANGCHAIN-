#This script enables users to upload a PDF document and ask questions about its 
content using an AI language model (Groq's LLaMA 3 via LangChain). 
The PDF is processed, embedded, and queried through a 
Retrieval-Augmented Generation (RAG) pipeline.

---------

from langchain.llms.base import LLM
from groq import Groq
from typing import List, Optional, Any, Mapping
from pydantic import Field
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from PyPDF2 import PdfReader



class GroqLLM(LLM):
    client: Any = Field(default_factory=lambda: Groq(api_key="#groq api key"))
    model_name: str = "llama3-8b-8192" 
    system_prompt: str = "You are a helpful assistant."

    @property
    def _llm_type(self) -> str:
        return "groq"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt},
            ],
            max_tokens=512,
            temperature=0.7,
            stop=stop,
        )
        return response.choices[0].message.content

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        return {"model_name": self.model_name}



def extract_pdf(pdf):
    reader=PdfReader(pdf)
    full_text=""
    for page in reader.pages:
        full_text+=page.extract_text()
    return full_text

def main():
    pdf=input("inset path")
    document_text=extract_pdf(pdf)
    text_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=10)
    chunks=text_splitter.split_text(document_text)

    embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = FAISS.from_texts(chunks,embedding=embeddings)
    retriever=vectorstore.as_retriever()

    llm=GroqLLM()
    qna_chain=RetrievalQA.from_chain_type(llm=llm,retriever=retriever)


    while True:
        question=input("enter question")
        if question.lower==exit:
            break
        answer=qna_chain.run(question)
        print("\nANSWER : ",answer,"\n")

if __name__=="__main__":
    main()
            
